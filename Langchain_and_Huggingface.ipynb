{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TKPbLSeKbH1"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For API Calls\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "yreTwb39KlZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment Secret Key\n",
        "from google.colab import userdata\n",
        "sec_key=userdata.get(\"HF_TOKEN\")\n",
        "print(sec_key)"
      ],
      "metadata": {
        "id": "XZdadycuK5kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "QpkeO0mUMtxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "sec_key=userdata.get(\"HUGGINGFACEHUBTOKEN\")\n",
        "print(sec_key)"
      ],
      "metadata": {
        "id": "j8QZPINdNnu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUBTOKEN\"]=sec_key"
      ],
      "metadata": {
        "id": "67fn_fUmNPyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    temperature=0.7,  # Pass temperature directly\n",
        "    model_kwargs={\n",
        "        \"max_length\": 128,  # Specify other parameters inside model_kwargs\n",
        "        \"token\": sec_key    # Pass the API key here\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "E1r3KBpbNrAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"What is ML?\")"
      ],
      "metadata": {
        "id": "f_DFJp61OqR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "Fg98XijLPToZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who won the 2011 Cricket World Cup in 2011?\"\n",
        "template = \"\"\"Question: {question}\n",
        "Answer: Let;s think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template = template, input_variables=[\"question\"])\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "Ei-cQsCNP-xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm = llm, prompt = prompt)\n",
        "print(llm_chain.invoke(question))"
      ],
      "metadata": {
        "id": "vHL7Y_5KQFyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging face Pipeline"
      ],
      "metadata": {
        "id": "YK1YDigUS5lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "lkXrFAryQ0PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "BKRIb0B5TP5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer, max_new_tokens = 100)\n",
        "hf = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "K-vCN38fURak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf"
      ],
      "metadata": {
        "id": "IWlW39IWUdZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(\"Huggingface is a company\")"
      ],
      "metadata": {
        "id": "T4i41IVdU5zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2W6VT1EUU_F7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}